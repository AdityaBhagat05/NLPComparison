{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h7oPS1XdTl6h",
        "outputId": "fbc9d07d-6304-4bd6-c0e2-60bab305f1eb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ PKL files loaded successfully.\n",
            "✅ Example normalized: ['In', 'talks', 'with', 'Mr.', 'yoshoda', ',', 'Chinese', 'leaders', 'expressed', 'no', 'regret', 'for', 'the', 'killings', ',', 'and', 'even', 'suggested', 'that', 'the', 'Hindupur', 'was', 'prominently', 'involved', '*-1', 'in', 'the', 'demonstrations', 'this', 'spring', '.']\n",
            "Epoch 1/10 - Loss: 13219.18774368207\n",
            "Epoch 2/10 - Loss: 6901.332701245044\n",
            "Epoch 3/10 - Loss: 5161.281156843302\n",
            "Epoch 4/10 - Loss: 4015.003908773549\n",
            "Epoch 5/10 - Loss: 3147.5030995552684\n",
            "Epoch 6/10 - Loss: 2500.6087099696147\n",
            "Epoch 7/10 - Loss: 2098.481982845162\n",
            "Epoch 8/10 - Loss: 1803.6427189096707\n",
            "Epoch 9/10 - Loss: 1604.2614158968493\n",
            "Epoch 10/10 - Loss: 1363.4789129328992\n",
            "\n",
            "✅ FINAL METRICS\n",
            "Accuracy: 0.9288397015373829\n",
            "Macro F1: 0.8557783423772984\n",
            "\n",
            "✅ Test Inference:\n",
            "[('Aditya', 'NNP'), ('visited', 'VBD'), ('Mumbai', 'NNP'), ('yesterday', 'NN')]\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import pickle\n",
        "import random\n",
        "import spacy\n",
        "from spacy.training import Example\n",
        "from spacy.tokens import Doc\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "\n",
        "\n",
        "def load_pkl(path):\n",
        "    with open(path, \"rb\") as f:\n",
        "        return pickle.load(f)\n",
        "\n",
        "train_raw = load_pkl(\"train_pos_data.pkl\")\n",
        "test_raw  = load_pkl(\"test_pos_data.pkl\")\n",
        "\n",
        "print(\"✅ PKL files loaded successfully.\")\n",
        "\n",
        "\n",
        "\n",
        "def normalize(data):\n",
        "    sentences = []\n",
        "    tags = []\n",
        "\n",
        "    for sample in data:\n",
        "        words = [w for (w, t) in sample]\n",
        "        pos   = [t for (w, t) in sample]\n",
        "\n",
        "        sentences.append(words)   # ✅ store as list of words\n",
        "        tags.append(pos)\n",
        "\n",
        "    return sentences, tags\n",
        "\n",
        "\n",
        "train_sentences, train_tags = normalize(train_raw)\n",
        "test_sentences,  test_tags  = normalize(test_raw)\n",
        "\n",
        "print(\"✅ Example normalized:\", train_sentences[0])\n",
        "\n",
        "\n",
        "\n",
        "nlp = spacy.blank(\"en\")\n",
        "\n",
        "def custom_tokenizer(nlp, text):\n",
        "    words = text.split(\" \")\n",
        "    spaces = [True] * len(words)\n",
        "    spaces[-1] = False\n",
        "    return Doc(nlp.vocab, words=words, spaces=spaces)\n",
        "\n",
        "nlp.tokenizer = lambda text: custom_tokenizer(nlp, text)\n",
        "\n",
        "tagger = nlp.add_pipe(\"tagger\")\n",
        "\n",
        "\n",
        "for seq in train_tags:\n",
        "    for tag in seq:\n",
        "        tagger.add_label(tag)\n",
        "\n",
        "nlp.initialize()\n",
        "\n",
        "\n",
        "\n",
        "train_data = [(\" \".join(words), {\"tags\": t}) for words, t in zip(train_sentences, train_tags)]\n",
        "test_data  = [(\" \".join(words), {\"tags\": t}) for words, t in zip(test_sentences,  test_tags)]\n",
        "\n",
        "for epoch in range(10):\n",
        "    random.shuffle(train_data)\n",
        "    losses = {}\n",
        "\n",
        "    for text, ann in train_data:\n",
        "        doc = nlp.make_doc(text)\n",
        "        example = Example.from_dict(doc, ann)\n",
        "        nlp.update([example], losses=losses)\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/10 - Loss:\", losses[\"tagger\"])\n",
        "\n",
        "\n",
        "\n",
        "true_labels = []\n",
        "pred_labels = []\n",
        "\n",
        "for sent_words, gold_tags in zip(test_sentences, test_tags):\n",
        "    text = \" \".join(sent_words)\n",
        "    doc = nlp(text)\n",
        "\n",
        "    preds = [tok.tag_ for tok in doc]\n",
        "\n",
        "    assert len(preds) == len(gold_tags), \"❌ Token mismatch detected!\"\n",
        "\n",
        "    true_labels.extend(gold_tags)\n",
        "    pred_labels.extend(preds)\n",
        "\n",
        "acc = accuracy_score(true_labels, pred_labels)\n",
        "f1 = f1_score(true_labels, pred_labels, average='macro')\n",
        "\n",
        "print(\"\\n✅ FINAL METRICS\")\n",
        "print(\"Accuracy:\", acc)\n",
        "print(\"Macro F1:\", f1)\n",
        "\n",
        "\n",
        "\n",
        "def predict_spacy(text):\n",
        "    doc = nlp(text)\n",
        "    return [(tok.text, tok.tag_) for tok in doc]\n",
        "\n",
        "print(\"\\n✅ Test Inference:\")\n",
        "print(predict_spacy(\"Aditya visited Mumbai yesterday\"))\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
